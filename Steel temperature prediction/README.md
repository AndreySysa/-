# Предсказание температуры стали.

[ipynb](https://github.com/AndreySysa/Portfolio/blob/main/Steel%20temperature%20prediction/Steel%20temperature%20prediction.ipynb)[^1]


Перед нами стояла задача: 
- для металлургического комбината построить модель, которая предскажет температуру стали.
***
Нам были предоставлены:
- Краткое описание этапа обработки стали.
- Данные, состоящие из файлов формата `csv`, полученных из разных источников:
- - `data_arc_new.csv` — данные об электродах;
- - `data_bulk_new.csv` — данные о подаче сыпучих материалов (объём);
- - `data_bulk_time_new.csv` — данные о подаче сыпучих материалов (время);
- - `data_gas_new.csv` — данные о продувке сплава газом;
- - `data_temp_new.csv` — результаты измерения температуры;
- - `data_wire_new.csv` — данные о проволочных материалах (объём);
- - `data_wire_time_new.csv` — данные о проволочных материалах (время).

Во всех файлах столбец `key` содержит номер партии.
***
Для выполнения поставленной задачи использовались следующие библиотеки и модули:
- os: модуль для работы с операционной системой (файлы и папки)
- optuna: библиотека для оптимизации гиперпараметров моделей
- numpy: библиотека для работы с многомерными массивами и математическими функциями
- pandas: библиотека для работы с данными в виде таблиц (DataFrame)
- seaborn: библиотека для визуализации данных
- plotly.express: библиотека для создания интерактивных графиков и визуализаций
- matplotlib.pyplot: библиотека для создания статических графиков и визуализаций
- time: модуль для работы со временем
- sklearn.svm.SVR: класс для обучения модели методом опорных векторов (Support Vector Regression)
- lightgbm.LGBMRegressor: класс для обучения градиентного бустинга на деревьях (LightGBM)
- catboost.CatBoostRegressor: класс для обучения градиентного бустинга на деревьях (CatBoost)
- sklearn.dummy.DummyRegressor: класс для обучения простых базовых моделей (Dummy)
- sklearn.pipeline.make_pipeline: функция для создания конвейера из нескольких этапов обработки данных и моделей
- sklearn.preprocessing.StandardScaler: класс для стандартизации данных (Standardization)
- sklearn.linear_model.LinearRegression: класс для обучения линейной регрессии
- sklearn.ensemble.RandomForestRegressor: класс для обучения случайного леса (Random Forest)
- sklearn.model_selection.train_test_split: функция для разделения данных на обучающую и тестовую выборки
- sklearn.metrics.mean_absolute_error: функция для вычисления средней абсолютной ошибки (Mean Absolute Error)
- sklearn.model_selection.KFold: класс для разделения данных на фолды в кросс-валидации (KFold)
- sklearn.model_selection.cross_val_score: функция для оценки модели с использованием кросс-валидации
***
По требованиям заказчика.
- был зафиксирован параметр установки начального состояния генератора псевдослучайных чисел `RANDOM_STATE = 140823`
- целевая метрика $МАЕ\ngtr 6.8$
***
 В ходе предобработки данных были добавлены новые признаки.

 Данные в таблицах были агрегированны по ключу.

 В ходе проведения анализа данных были выявлены моменты, которые уточнялись и согласовывались с представителем заказчика (Был составлен список вопросов)
***
После уточнения информациии дообработали таблички. (удалили редкие (единичные) случаи и аномальные значения.
***
Избавились от мультиколлинеарности.

Прямой зависимости целевого признака от какого-либо параметра нет. Без построения модели нельзя сказать от чего зависит температура на заключительном этапе.
***
Для обучения моделей подготовили выборки (обучающую и тестовую c разбивкой $\frac{2}{3}$ и $\frac{1}{3}$ соответственно). 

Выделили целевой признак и функциональные признаки.

Так как признаки имеют разную размерность обучаили и провели стандартизацию на тренировочных данных, а затем провели ту же стандартизацию на тестовых данных.

Вычислили среднюю абсолютную ошибку (MAE=7.88) для модели, которая всегда предсказывает среднее значение целевой переменной. Получили таким образом базовую метрику для сравнения с другими моделями. Это позволило оценить, насколько хорошо другие модели справляются с прогнозированием по сравнению с этой "наивной" моделью.

Cоздали объект `KFold` для разделения датасета для кросс-валидации.

- LinearRegression() (линейная регрессия) выдала $МАЕ = 6.55$
- Случайный лес RandomForestRegressor - $МАЕ = 6.43$
- Градиентный бустинг LightGBM - $МАЕ = 6.25$
- Градиентный бустинг CatBoostRegressor - $МАЕ = 6.28$
- Support Vector Regression - $МАЕ = 6.42$

Для подбора гиперпараметров моделей использовали библиотеку для автоматической оптимизации параметров модели `Optuna`. Производили подбор поэтапно уменьшая интервалы параметров для подбора, используя для этих целей визуализацию (таблицы с параметрами моделей, показавших лучшую метрику при подборе параметров, графики важности гиперпараметров, графики истории оптимизации и графики контура для разных параметров)

Все модели показали метрику лучше, чем "наивная" модель. Более того они удовлетворяют требованиям заказчика $МАЕ\ngtr 6.8$
***
Проведено тестирование лучшей модели (на отложеной выборке). Получена $МАЕ = 6.62$, что удовлетворяют требованиям заказчика $МАЕ\ngtr 6.8$. Дополнительно было проведено сравнение с простой моделью регрессии DummyRegressor, выдавшей $МАЕ = 8.69$.

***
Было проведено исследование степени влияния признаков на целевой показатель.

- Наибольшую важность для целевой переменной имеют признаки:
- - `temperature_first_measurement` - температура в начале процесса обработки.
- - `energy_consumption` - потребляемая в процессе обработки энергия.
- - `time_between_measurements` - сумарное время нагрева.

[^1]:Для корректного просмотра Jupyter notebook с работающим содержанием, пожалуйста, используйте следующую ссылку:
[Предсказание температуры стали](https://nbviewer.jupyter.org/github/AndreySysa/Portfolio/blob/main/Steel%20temperature%20prediction/Steel%20temperature%20prediction.ipynb)
Просто кликните на ссылку, и nbviewer отобразит notebook с интерактивными ссылками.

